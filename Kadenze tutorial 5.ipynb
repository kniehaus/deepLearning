{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNS\n",
    "\n",
    "Written by KN, Feb-2017\n",
    "\n",
    "Based largely upon: https://github.com/pkmital/CADL/blob/master/session-5/lecture-5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, _ = urllib.request.urlretrieve('https://www.gutenberg.org/cache/epub/11/pg11.txt', 'alice.txt')\n",
    "with open(f, 'r') as fp:\n",
    "    txt = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x03',\n",
       " '\\x04',\n",
       " '\\x05',\n",
       " '\\x06',\n",
       " '\\x07',\n",
       " '\\x08',\n",
       " '\\t',\n",
       " '\\n',\n",
       " '\\x0b',\n",
       " '\\x0c',\n",
       " '\\r',\n",
       " '\\x0e',\n",
       " '\\x0f',\n",
       " '\\x10',\n",
       " '\\x11',\n",
       " '\\x12',\n",
       " '\\x13',\n",
       " '\\x14',\n",
       " '\\x15',\n",
       " '\\x16',\n",
       " '\\x17',\n",
       " '\\x18',\n",
       " '\\x19',\n",
       " '\\x1a',\n",
       " '\\x1b',\n",
       " '\\x1c',\n",
       " '\\x1d',\n",
       " '\\x1e',\n",
       " '\\x1f',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\x7f',\n",
       " '\\x80',\n",
       " '\\x81',\n",
       " '\\x82',\n",
       " '\\x83',\n",
       " '\\x84',\n",
       " '\\x85',\n",
       " '\\x86',\n",
       " '\\x87',\n",
       " '\\x88',\n",
       " '\\x89',\n",
       " '\\x8a',\n",
       " '\\x8b',\n",
       " '\\x8c',\n",
       " '\\x8d',\n",
       " '\\x8e',\n",
       " '\\x8f',\n",
       " '\\x90',\n",
       " '\\x91',\n",
       " '\\x92',\n",
       " '\\x93',\n",
       " '\\x94',\n",
       " '\\x95',\n",
       " '\\x96',\n",
       " '\\x97',\n",
       " '\\x98',\n",
       " '\\x99',\n",
       " '\\x9a',\n",
       " '\\x9b',\n",
       " '\\x9c',\n",
       " '\\x9d',\n",
       " '\\x9e',\n",
       " '\\x9f',\n",
       " '\\xa0',\n",
       " '\\xa1',\n",
       " '\\xa2',\n",
       " '\\xa3',\n",
       " '\\xa4',\n",
       " '\\xa5',\n",
       " '\\xa6',\n",
       " '\\xa7',\n",
       " '\\xa8',\n",
       " '\\xa9',\n",
       " '\\xaa',\n",
       " '\\xab',\n",
       " '\\xac',\n",
       " '\\xad',\n",
       " '\\xae',\n",
       " '\\xaf',\n",
       " '\\xb0',\n",
       " '\\xb1',\n",
       " '\\xb2',\n",
       " '\\xb3',\n",
       " '\\xb4',\n",
       " '\\xb5',\n",
       " '\\xb6',\n",
       " '\\xb7',\n",
       " '\\xb8',\n",
       " '\\xb9',\n",
       " '\\xba',\n",
       " '\\xbb',\n",
       " '\\xbc',\n",
       " '\\xbd',\n",
       " '\\xbe',\n",
       " '\\xbf',\n",
       " '\\xc0',\n",
       " '\\xc1',\n",
       " '\\xc2',\n",
       " '\\xc3',\n",
       " '\\xc4',\n",
       " '\\xc5',\n",
       " '\\xc6',\n",
       " '\\xc7',\n",
       " '\\xc8',\n",
       " '\\xc9',\n",
       " '\\xca',\n",
       " '\\xcb',\n",
       " '\\xcc',\n",
       " '\\xcd',\n",
       " '\\xce',\n",
       " '\\xcf',\n",
       " '\\xd0',\n",
       " '\\xd1',\n",
       " '\\xd2',\n",
       " '\\xd3',\n",
       " '\\xd4',\n",
       " '\\xd5',\n",
       " '\\xd6',\n",
       " '\\xd7',\n",
       " '\\xd8',\n",
       " '\\xd9',\n",
       " '\\xda',\n",
       " '\\xdb',\n",
       " '\\xdc',\n",
       " '\\xdd',\n",
       " '\\xde',\n",
       " '\\xdf',\n",
       " '\\xe0',\n",
       " '\\xe1',\n",
       " '\\xe2',\n",
       " '\\xe3',\n",
       " '\\xe4',\n",
       " '\\xe5',\n",
       " '\\xe6',\n",
       " '\\xe7',\n",
       " '\\xe8',\n",
       " '\\xe9',\n",
       " '\\xea',\n",
       " '\\xeb',\n",
       " '\\xec',\n",
       " '\\xed',\n",
       " '\\xee',\n",
       " '\\xef',\n",
       " '\\xf0',\n",
       " '\\xf1',\n",
       " '\\xf2',\n",
       " '\\xf3',\n",
       " '\\xf4',\n",
       " '\\xf5',\n",
       " '\\xf6',\n",
       " '\\xf7',\n",
       " '\\xf8',\n",
       " '\\xf9',\n",
       " '\\xfa',\n",
       " '\\xfb',\n",
       " '\\xfc',\n",
       " '\\xfd',\n",
       " '\\xfe',\n",
       " '\\xff'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60422, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(txt))\n",
    "len(txt), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = dict(zip(vocab, range(len(vocab))))\n",
    "decoder = dict(zip(range(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000:0\n",
      "�:1\n",
      "\u0004:2\n",
      "�:3\n",
      "\b:4\n",
      "�:5\n",
      "\f",
      ":6\n",
      "�:7\n",
      "\u0010:8\n",
      "�:9\n",
      "\u0014:10\n",
      "�:11\n",
      "\u0018:12\n",
      "�:13\n",
      "\u001c",
      ":14\n",
      "�:15\n",
      " :16\n",
      "�:17\n",
      "$:18\n",
      "�:19\n",
      "(:20\n",
      "�:21\n",
      ",:22\n",
      "�:23\n",
      "0:24\n",
      "�:25\n",
      "4:26\n",
      "�:27\n",
      "8:28\n",
      "�:29\n",
      "<:30\n",
      "�:31\n",
      "@:32\n",
      "�:33\n",
      "D:34\n",
      "�:35\n",
      "H:36\n",
      "�:37\n",
      "L:38\n",
      "�:39\n",
      "P:40\n",
      "�:41\n",
      "T:42\n",
      "�:43\n",
      "X:44\n",
      "�:45\n",
      "\\:46\n",
      "�:47\n",
      "`:48\n",
      "�:49\n",
      "d:50\n",
      "�:51\n",
      "h:52\n",
      "�:53\n",
      "l:54\n",
      "�:55\n",
      "p:56\n",
      "�:57\n",
      "t:58\n",
      "�:59\n",
      "x:60\n",
      "�:61\n",
      "|:62\n",
      "�:63\n",
      "�:64\n",
      "\u0003:65\n",
      "�:66\n",
      "\u0007:67\n",
      "�:68\n",
      "\u000b",
      ":69\n",
      "�:70\n",
      "\u000f:71\n",
      "�:72\n",
      "\u0013:73\n",
      "�:74\n",
      "\u0017:75\n",
      "�:76\n",
      "\u001b:77\n",
      "�:78\n",
      "\u001f:79\n",
      "�:80\n",
      "#:81\n",
      "�:82\n",
      "':83\n",
      "�:84\n",
      "+:85\n",
      "�:86\n",
      "/:87\n",
      "�:88\n",
      "3:89\n",
      "�:90\n",
      "7:91\n",
      "�:92\n",
      ";:93\n",
      "�:94\n",
      "?:95\n",
      "�:96\n",
      "C:97\n",
      "�:98\n",
      "G:99\n",
      "�:100\n",
      "K:101\n",
      "�:102\n",
      "O:103\n",
      "�:104\n",
      "S:105\n",
      "�:106\n",
      "W:107\n",
      "�:108\n",
      "[:109\n",
      "�:110\n",
      "_:111\n",
      "�:112\n",
      "c:113\n",
      "�:114\n",
      "g:115\n",
      "�:116\n",
      "k:117\n",
      "�:118\n",
      "o:119\n",
      "�:120\n",
      "s:121\n",
      "�:122\n",
      "w:123\n",
      "�:124\n",
      "{:125\n",
      "�:126\n",
      ":127\n",
      "�:128\n",
      "\u0002:129\n",
      "�:130\n",
      "\u0006:131\n",
      "�:132\n",
      "\n",
      ":133\n",
      "�:134\n",
      "\u000e:135\n",
      "�:136\n",
      "\u0012:137\n",
      "�:138\n",
      "\u0016:139\n",
      "�:140\n",
      "\u001a:141\n",
      "�:142\n",
      "\u001e",
      ":143\n",
      "�:144\n",
      "\":145\n",
      "�:146\n",
      "&:147\n",
      "�:148\n",
      "*:149\n",
      "�:150\n",
      ".:151\n",
      "�:152\n",
      "2:153\n",
      "�:154\n",
      "6:155\n",
      "�:156\n",
      "::157\n",
      "�:158\n",
      ">:159\n",
      "�:160\n",
      "B:161\n",
      "�:162\n",
      "F:163\n",
      "�:164\n",
      "J:165\n",
      "�:166\n",
      "N:167\n",
      "�:168\n",
      "R:169\n",
      "�:170\n",
      "V:171\n",
      "�:172\n",
      "Z:173\n",
      "�:174\n",
      "^:175\n",
      "�:176\n",
      "b:177\n",
      "�:178\n",
      "f:179\n",
      "�:180\n",
      "j:181\n",
      "�:182\n",
      "n:183\n",
      "�:184\n",
      "r:185\n",
      "�:186\n",
      "v:187\n",
      "�:188\n",
      "z:189\n",
      "�:190\n",
      "~:191\n",
      "\u0001:192\n",
      "�:193\n",
      "\u0005:194\n",
      "�:195\n",
      "\t:196\n",
      "�:197\n",
      "\r",
      ":198\n",
      "�:199\n",
      "\u0011:200\n",
      "�:201\n",
      "\u0015:202\n",
      "�:203\n",
      "\u0019:204\n",
      "�:205\n",
      "\u001d",
      ":206\n",
      "�:207\n",
      "!:208\n",
      "�:209\n",
      "%:210\n",
      "�:211\n",
      "):212\n",
      "�:213\n",
      "-:214\n",
      "�:215\n",
      "1:216\n",
      "�:217\n",
      "5:218\n",
      "�:219\n",
      "9:220\n",
      "�:221\n",
      "=:222\n",
      "�:223\n",
      "A:224\n",
      "�:225\n",
      "E:226\n",
      "�:227\n",
      "I:228\n",
      "�:229\n",
      "M:230\n",
      "�:231\n",
      "Q:232\n",
      "�:233\n",
      "U:234\n",
      "�:235\n",
      "Y:236\n",
      "�:237\n",
      "]:238\n",
      "�:239\n",
      "a:240\n",
      "�:241\n",
      "e:242\n",
      "�:243\n",
      "i:244\n",
      "�:245\n",
      "m:246\n",
      "�:247\n",
      "q:248\n",
      "�:249\n",
      "u:250\n",
      "�:251\n",
      "y:252\n",
      "�:253\n",
      "}:254\n",
      "�:255\n"
     ]
    }
   ],
   "source": [
    "for k in encoder.keys():\n",
    "    print('{0}:{1}'.format(k,encoder[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model params\n",
    "# Number of sequences in a mini batch\n",
    "batch_size = 100\n",
    "\n",
    "# Number of characters in a sequence\n",
    "sequence_length = 100\n",
    "\n",
    "# Number of cells in LSTM layer\n",
    "n_cells = 256\n",
    "\n",
    "# Number of LSTM layers\n",
    "n_layers = 2\n",
    "\n",
    "# Total number of characters in the one-hot encoding\n",
    "n_chars = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs: batch size x size of sequence length\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length], name='X')\n",
    "\n",
    "# Outputs\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 100, 256]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels into embeddings through TF functionality:\n",
    "# (vs. making one-hot encodings ourselves)\n",
    "\n",
    "# Variable to go from one-hot representation to our LSTM cells\n",
    "embedding = tf.get_variable(\"embedding\", [n_chars, n_cells])\n",
    "\n",
    "# Use tensorflow's embedding lookup to look up the ids in X\n",
    "Xs = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "# Can see: resulting lookups are concatenated into a dense tensor\n",
    "print(Xs.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.split?\n",
    "# tf.split(split_dim, num_split, value)\n",
    "# splits (value) along (split_dim) into (num_split) tensors [must divide out equally]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.squeeze?\n",
    "# tf.squeeze(input, squeeze_dims) --> \n",
    "# returns same tensor with all dimensions of size 1 listed in (squeeze_dims) removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"split_3:0\", shape=(?, 1, 256), dtype=float32)\n",
      "[None, 256]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Need to make slices of sequences\n",
    "# Let's create a name scope for the operations to clean things up in our graph\n",
    "print(tf.split(1, sequence_length, Xs)[0])\n",
    "with tf.name_scope('reslice'):\n",
    "    Xs_list = [tf.squeeze(seq, [1]) for seq in tf.split(1, sequence_length, Xs)]  \n",
    "print(Xs_list[0].get_shape().as_list())\n",
    "print(len(Xs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create single recurrent layer\n",
    "cells_single = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_cells, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize state\n",
    "initial_state_single = cells_single.zero_state(tf.shape(X)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For multiple layers (we have specified multiple layers here)\n",
    "cells = tf.nn.rnn_cell.MultiRNNCell([cells_single] * n_layers, state_is_tuple=True)\n",
    "initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of outputs of every element in input sequence.\n",
    "\n",
    "# Output will be batch_size x n_cells\n",
    "# state will be (n_cells' memory, output)\n",
    "outputs, state = tf.nn.rnn(cells, Xs_list, initial_state=initial_state)\n",
    "\n",
    "# Stack all outputs for every cell\n",
    "outputs_flat = tf.reshape(tf.concat(1, outputs), [-1, n_cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do prediction step\n",
    "with tf.variable_scope('prediction'):\n",
    "    W = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[n_cells, n_chars],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    b = tf.get_variable(\n",
    "        \"b\",\n",
    "        shape=[n_chars],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "    # Find the output prediction of every single character in our minibatch\n",
    "    # we denote the pre-activation prediction, logits.\n",
    "    logits = tf.matmul(outputs_flat, W) + b\n",
    "\n",
    "    # We get the probabilistic version by calculating the softmax of this\n",
    "    probs = tf.nn.softmax(logits)\n",
    "\n",
    "    # And then we can find the index of maximum probability\n",
    "    Y_pred = tf.argmax(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make loss function\n",
    "with tf.variable_scope('loss'):\n",
    "    # Compute mean cross entropy loss for each output.\n",
    "    Y_true_flat = tf.reshape(tf.concat(1, Y), [-1])   # why add the 1??\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, Y_true_flat)\n",
    "    mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clip the gradient\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    gradients = []\n",
    "    clip = tf.constant(5.0, name=\"clip\")\n",
    "    for grad, var in optimizer.compute_gradients(mean_loss):\n",
    "        gradients.append((tf.clip_by_value(grad, -clip, clip), var))\n",
    "    updates = optimizer.apply_gradients(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train!\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "cursor = 0\n",
    "it_i = 0\n",
    "while True:\n",
    "    Xs, Ys = [], []\n",
    "    for batch_i in range(batch_size):\n",
    "        if (cursor + sequence_length) >= len(txt) - sequence_length - 1:\n",
    "            cursor = 0\n",
    "        Xs.append([encoder[ch]\n",
    "                   for ch in txt[cursor:cursor + sequence_length]])\n",
    "        Ys.append([encoder[ch]\n",
    "                   for ch in txt[cursor + 1: cursor + sequence_length + 1]])\n",
    "\n",
    "        cursor = (cursor + sequence_length)\n",
    "    Xs = np.array(Xs).astype(np.int32)\n",
    "    Ys = np.array(Ys).astype(np.int32)\n",
    "\n",
    "    loss_val, _ = sess.run([mean_loss, updates],\n",
    "                           feed_dict={X: Xs, Y: Ys})\n",
    "    print(it_i, loss_val)\n",
    "\n",
    "    if it_i % 500 == 0:\n",
    "        p = sess.run([Y_pred], feed_dict={X: Xs})[0]\n",
    "        preds = [decoder[p_i] for p_i in p]\n",
    "        print(\"\".join(preds).split('\\n'))\n",
    "\n",
    "    it_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
