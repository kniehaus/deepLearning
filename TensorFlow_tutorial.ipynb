{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Introduction\n",
    "November 2016, KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import more modules\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "Going through simple tutorial first, just to go through https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load up MNIST data\n",
    "mnist = input_data.read_data_sets(\"/Users/kateniehaus/General_code/data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Illustrate raw data\n",
    "print('Dimensions of training set: {0}'.format(np.shape(mnist.train.images)))\n",
    "exInd = 5\n",
    "exImage = mnist.train.images[exInd,:].reshape(28,-1)  # 28x28 pixels in each image\n",
    "plt.pcolormesh(exImage, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set-up tensor flow variables\n",
    "dimen = 28*28  # Size of images and therefore the imput vectors\n",
    "numClasses = 10   # Number of possible digits (0-9)\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, dimen])\n",
    "# None means dimension can be of any length\n",
    "# Doing this to create placeholder for each 28x28 pixel image\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, numClasses])\n",
    "# one-hot vector indivating true digit class\n",
    "\n",
    "# Variables\n",
    "# (model parameters are usually tensorflow variables)\n",
    "# Need to initialize values for variables\n",
    "w = tf.Variable(tf.zeros([dimen,numClasses]))\n",
    "b = tf.Variable(tf.zeros([numClasses]))\n",
    "\n",
    "# Softmax\n",
    "y = tf.nn.softmax(tf.matmul(x,w) + b)\n",
    "\n",
    "# Cross-entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true*tf.log(y), reduction_indices=[1]))\n",
    "# tf.reduce_mean gets mean cross-entropy for all samples\n",
    "# more numerically stable:\n",
    "#cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_true))\n",
    "# but then softmax already taken, so would remove from y function \n",
    "\n",
    "# Need to initialize variables and launch a session\n",
    "# Session is connection to C++ code backend to do computation\n",
    "# First, create graph and then launch in a session\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize\n",
    "learnRate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(learnRate).minimize(cross_entropy)\n",
    "# When this step is run, it will apply gradient descent updates to params\n",
    "\n",
    "# Running training step\n",
    "numSteps = 1000\n",
    "batchSize = 100\n",
    "for i in range(numSteps):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batchSize)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "    # feed_dict replaces placeholders with training examples\n",
    "    # (can use feed_dict to replace any tensor in computation graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at performance in training data\n",
    "predLabels = tf.argmax(y,1)\n",
    "trueLabels = tf.argmax(y_true,1)\n",
    "\n",
    "ifCorrect = tf.equal(predLabels, trueLabels)\n",
    "\n",
    "accFunc = tf.reduce_mean(tf.cast(ifCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at performance in test data\n",
    "\n",
    "testPerf_acc = sess.run(accFunc, feed_dict={x:mnist.test.images, y_true:mnist.test.labels})\n",
    "print('Test accuracy: {0:.2f}%'.format(100*testPerf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With interactive session\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.48%\n"
     ]
    }
   ],
   "source": [
    "# Or, can use an interactive session\n",
    "# With interactive session, can interleave graph-building and graph-running operations\n",
    "sess2 = tf.InteractiveSession()\n",
    "\n",
    "# (As above)\n",
    "dimen = 28*28  # Size of images and therefore the imput vectors\n",
    "numClasses = 10   # Number of possible digits (0-9)\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, dimen])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, numClasses])\n",
    "# Variables\n",
    "w = tf.Variable(tf.zeros([dimen,numClasses]))\n",
    "b = tf.Variable(tf.zeros([numClasses]))\n",
    "# Regression model\n",
    "y = tf.matmul(x,w) + b\n",
    "# Cross-entropy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "sess2.run(tf.initialize_all_variables())\n",
    "\n",
    "# Optimize\n",
    "learnRate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(learnRate).minimize(cross_entropy)\n",
    "\n",
    "# Running training step\n",
    "numSteps = 1000\n",
    "batchSize = 100\n",
    "for i in range(numSteps):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "# Look at performance\n",
    "predLabels = tf.argmax(y,1)\n",
    "trueLabels = tf.argmax(y_,1)\n",
    "\n",
    "ifCorrect = tf.equal(predLabels, trueLabels)\n",
    "accFunc = tf.reduce_mean(tf.cast(ifCorrect, tf.float32))\n",
    "\n",
    "testPerf_acc = accFunc.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "print('Test accuracy: {0:.2f}%'.format(100*testPerf_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple conv net\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html\n",
    "\n",
    "Remember: CNN is a bunch of convolution layers + ReLU applied\n",
    "(where the filters [= conv layers = kernels = filters = feature detectors] are learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make some functions \n",
    "\n",
    "\n",
    "def sizeNextLayer(w,F=3,P=1,S=1):\n",
    "    \"\"\" Returns dimension expected, given parameters for conv layer\n",
    "    \n",
    "    w: int\n",
    "        Width of input\n",
    "        \n",
    "    F: int (default=3)\n",
    "        Receptive field of neuron\n",
    "        \n",
    "    P: int (default=1)\n",
    "        Number of 0s to add for padding of input\n",
    "        \n",
    "    S: int (default=1)\n",
    "        Stride size\n",
    "        \n",
    "    \"\"\"\n",
    "    return (w-F+2*P)/np.float(S) + 1\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Want weight initializations to have noise for symmetry breaking\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    # outputs draws from truncated normal w/ mean=0 (default std=1)\n",
    "    # shape should be 1-D\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Want positive bias for ReLU neurons to avoid dead neurons (i.e., never activate)\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W, S=1):\n",
    "    \"\"\"Want to set S=1 stride and add zero padding to keep output of same size as input\n",
    "    \n",
    "    Zero padding: means that when applying convolution at edges, pad with zeros \n",
    "    to enable the computation (vs. narrow convolution)\n",
    "    --> 'SAME' means set padding so output size=input size\n",
    "    \n",
    "    Stride size: amount to shift filter in each step\n",
    "    \n",
    "    x: 4D tensor\n",
    "        Input data (batch, height, width, channels)\n",
    "    \n",
    "    W: weights of neuron\n",
    "    \n",
    "    \"\"\"\n",
    "    return tf.nn.conv2d(x,W, strides=[S,S,S,S], padding='SAME')\n",
    "\n",
    "def max_pool_set(x, S=2, F=2):\n",
    "    \"\"\"Want to do max pooling over 2x2 blocks\n",
    "    (means, take max value contained within each 2x2 block)\n",
    "    \n",
    "    x: 4D tensor\n",
    "        Input data (batch, height, width, channels)\n",
    "        \n",
    "    S: int (default=2)\n",
    "        Stride size\n",
    "    \n",
    "    F: int (default=2)\n",
    "        Spatial extent\n",
    "    \n",
    "    Max pooling just operates spatially, so depth in terms of channels=1\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1,F,F,1], strides=[1,S,S,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set variables\n",
    "F = 1   # receptive field of neuron (i.e., patch size)\n",
    "d0 = 1  # (number input color channels)\n",
    "w0 = 28   # width of input (=height)\n",
    "K1 = 32  # number of filters in layer 1\n",
    "K2 = 64   # number of filters in layer 2\n",
    "K3 = 1024    # number of filters in FC layer 3\n",
    "# Padding, P, taken care of internally with TF\n",
    "# We already set S defaults in shortcut functions above\n",
    "S_m = 2  # max-pooling  \n",
    "F_m = 2  # max-pooling\n",
    "nC = 10    # number of classes in final labels\n",
    "\n",
    "# To look at for interest & playing:\n",
    "sizeNextLayer(w0,F=F,P=2,S=1)\n",
    "\n",
    "#w0*w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create first convolutional layer (using shortcut functions above)\n",
    "W_conv1 = weight_variable([F,F, d0, K1])\n",
    "b_conv1 = bias_variable([K1])\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, w0*w0])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, nC])\n",
    "\n",
    "# reshape images into 4d tensor of: [numExamples, width, height, num color channels]\n",
    "x_image = tf.reshape(x, [-1, w0, w0,d0])\n",
    "\n",
    "# convolve image with weight tensor and add bias\n",
    "conv1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "# apply ReLU function\n",
    "h_conv1 = tf.nn.relu(conv1)\n",
    "# max pool\n",
    "h_pool1 = max_pool_set(h_conv1, S=S_m, F=F_m)\n",
    "\n",
    "# --> so after max-pooling over 2x2 blocks, 'image' size is 14x14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create second convolutional layer\n",
    "\n",
    "W_conv2 = weight_variable([F,F, K1, K2])\n",
    "b_conv2 = bias_variable([K2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_set(h_conv2, S=S_m, F=F_m)\n",
    "\n",
    "# --> so after max-pooling over 2x2 blocks, 'image' size is 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create FC layer\n",
    "numMaxPools = 2\n",
    "currImSize = w0/(F_m*numMaxPools)\n",
    "\n",
    "W_fc1 = weight_variable([currImSize*currImSize*K2, K3])\n",
    "b_fc1 = bias_variable([K3])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, currImSize*currImSize*K2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "# Note: no scaling needed b/c handled behind the scenes by TF\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)   # make placeholder so can turn off during testing\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add readout layer\n",
    "\n",
    "W_fc2 = weight_variable([K3, nC])\n",
    "b_fc2 = bias_variable([nC])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2   #(same form as regression model above)\n",
    "# (for easy comparison, it was: y = tf.matmul(x,w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainRate = 1e-4\n",
    "\n",
    "\n",
    "# Loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "# (same form as regression above; for easy comparison, it was: \n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_)))\n",
    "\n",
    "# use ADAM optimizer (stochastic optimizer)\n",
    "train_step = tf.train.AdamOptimizer(trainRate).minimize(cross_entropy)\n",
    "\n",
    "# Look at performance\n",
    "predLabels = tf.argmax(y_conv,1)\n",
    "trueLabels = tf.argmax(y_,1)\n",
    "\n",
    "ifCorrect = tf.equal(predLabels, trueLabels)\n",
    "accFunc = tf.reduce_mean(tf.cast(ifCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training acc = 0.060\n",
      "Step 100, training acc = 0.860\n",
      "Step 200, training acc = 0.860\n",
      "Step 300, training acc = 0.920\n",
      "Step 400, training acc = 0.900\n",
      "Step 500, training acc = 0.960\n",
      "Step 600, training acc = 0.920\n",
      "Step 700, training acc = 0.980\n",
      "Step 800, training acc = 0.960\n",
      "Step 900, training acc = 0.920\n",
      "Step 1000, training acc = 0.980\n",
      "Step 1100, training acc = 0.980\n",
      "Step 1200, training acc = 0.940\n",
      "Step 1300, training acc = 0.960\n",
      "Step 1400, training acc = 0.960\n",
      "Step 1500, training acc = 0.960\n",
      "Step 1600, training acc = 0.940\n",
      "Step 1700, training acc = 0.960\n",
      "Step 1800, training acc = 0.940\n",
      "Step 1900, training acc = 0.980\n",
      "Step 2000, training acc = 0.980\n",
      "Step 2100, training acc = 0.960\n",
      "Step 2200, training acc = 0.940\n",
      "Step 2300, training acc = 1.000\n",
      "Step 2400, training acc = 0.940\n",
      "Step 2500, training acc = 1.000\n",
      "Step 2600, training acc = 0.960\n",
      "Step 2700, training acc = 1.000\n",
      "Step 2800, training acc = 0.980\n",
      "Step 2900, training acc = 1.000\n",
      "Step 3000, training acc = 0.960\n",
      "Step 3100, training acc = 1.000\n",
      "Step 3200, training acc = 0.980\n",
      "Step 3300, training acc = 1.000\n",
      "Step 3400, training acc = 0.980\n",
      "Step 3500, training acc = 0.980\n",
      "Step 3600, training acc = 1.000\n",
      "Step 3700, training acc = 1.000\n",
      "Step 3800, training acc = 1.000\n",
      "Step 3900, training acc = 1.000\n",
      "Step 4000, training acc = 0.980\n",
      "Step 4100, training acc = 1.000\n",
      "Step 4200, training acc = 0.980\n",
      "Step 4300, training acc = 0.980\n",
      "Step 4400, training acc = 0.980\n",
      "Step 4500, training acc = 1.000\n",
      "Step 4600, training acc = 0.980\n",
      "Step 4700, training acc = 1.000\n",
      "Step 4800, training acc = 0.960\n",
      "Step 4900, training acc = 1.000\n",
      "Step 5000, training acc = 1.000\n",
      "Step 5100, training acc = 1.000\n",
      "Step 5200, training acc = 1.000\n",
      "Step 5300, training acc = 0.980\n",
      "Step 5400, training acc = 1.000\n",
      "Step 5500, training acc = 1.000\n",
      "Step 5600, training acc = 0.980\n",
      "Step 5700, training acc = 0.960\n",
      "Step 5800, training acc = 1.000\n",
      "Step 5900, training acc = 0.980\n",
      "Step 6000, training acc = 1.000\n",
      "Step 6100, training acc = 1.000\n",
      "Step 6200, training acc = 1.000\n",
      "Step 6300, training acc = 1.000\n",
      "Step 6400, training acc = 0.980\n",
      "Step 6500, training acc = 1.000\n",
      "Step 6600, training acc = 1.000\n",
      "Step 6700, training acc = 1.000\n",
      "Step 6800, training acc = 0.980\n",
      "Step 6900, training acc = 1.000\n",
      "Step 7000, training acc = 0.960\n",
      "Step 7100, training acc = 1.000\n",
      "Step 7200, training acc = 1.000\n",
      "Step 7300, training acc = 0.980\n",
      "Step 7400, training acc = 1.000\n",
      "Step 7500, training acc = 1.000\n",
      "Step 7600, training acc = 1.000\n",
      "Step 7700, training acc = 1.000\n",
      "Step 7800, training acc = 1.000\n",
      "Step 7900, training acc = 1.000\n",
      "Step 8000, training acc = 1.000\n",
      "Step 8100, training acc = 1.000\n",
      "Step 8200, training acc = 1.000\n",
      "Step 8300, training acc = 1.000\n",
      "Step 8400, training acc = 1.000\n",
      "Step 8500, training acc = 1.000\n",
      "Step 8600, training acc = 0.980\n",
      "Step 8700, training acc = 1.000\n",
      "Step 8800, training acc = 1.000\n",
      "Step 8900, training acc = 1.000\n",
      "Step 9000, training acc = 1.000\n",
      "Step 9100, training acc = 0.980\n",
      "Step 9200, training acc = 1.000\n",
      "Step 9300, training acc = 1.000\n",
      "Step 9400, training acc = 0.980\n",
      "Step 9500, training acc = 0.960\n",
      "Step 9600, training acc = 1.000\n",
      "Step 9700, training acc = 1.000\n",
      "Step 9800, training acc = 1.000\n",
      "Step 9900, training acc = 1.000\n",
      "Step 10000, training acc = 1.000\n",
      "Step 10100, training acc = 1.000\n",
      "Step 10200, training acc = 1.000\n",
      "Step 10300, training acc = 1.000\n",
      "Step 10400, training acc = 1.000\n",
      "Step 10500, training acc = 1.000\n",
      "Step 10600, training acc = 1.000\n",
      "Step 10700, training acc = 1.000\n",
      "Step 10800, training acc = 1.000\n",
      "Step 10900, training acc = 1.000\n",
      "Step 11000, training acc = 1.000\n",
      "Step 11100, training acc = 1.000\n",
      "Step 11200, training acc = 1.000\n",
      "Step 11300, training acc = 1.000\n",
      "Step 11400, training acc = 1.000\n",
      "Step 11500, training acc = 1.000\n",
      "Step 11600, training acc = 1.000\n",
      "Step 11700, training acc = 1.000\n",
      "Step 11800, training acc = 1.000\n",
      "Step 11900, training acc = 1.000\n",
      "Step 12000, training acc = 1.000\n",
      "Step 12100, training acc = 1.000\n",
      "Step 12200, training acc = 1.000\n",
      "Step 12300, training acc = 1.000\n",
      "Step 12400, training acc = 1.000\n",
      "Step 12500, training acc = 0.980\n",
      "Step 12600, training acc = 1.000\n",
      "Step 12700, training acc = 1.000\n",
      "Step 12800, training acc = 0.980\n",
      "Step 12900, training acc = 1.000\n",
      "Step 13000, training acc = 1.000\n",
      "Step 13100, training acc = 0.980\n",
      "Step 13200, training acc = 1.000\n",
      "Step 13300, training acc = 1.000\n",
      "Step 13400, training acc = 1.000\n",
      "Step 13500, training acc = 1.000\n",
      "Step 13600, training acc = 1.000\n",
      "Step 13700, training acc = 1.000\n",
      "Step 13800, training acc = 1.000\n",
      "Step 13900, training acc = 1.000\n",
      "Step 14000, training acc = 1.000\n",
      "Step 14100, training acc = 1.000\n",
      "Step 14200, training acc = 1.000\n",
      "Step 14300, training acc = 1.000\n",
      "Step 14400, training acc = 1.000\n",
      "Step 14500, training acc = 1.000\n",
      "Step 14600, training acc = 1.000\n",
      "Step 14700, training acc = 1.000\n",
      "Step 14800, training acc = 1.000\n",
      "Step 14900, training acc = 1.000\n",
      "Step 15000, training acc = 1.000\n",
      "Step 15100, training acc = 1.000\n",
      "Step 15200, training acc = 1.000\n",
      "Step 15300, training acc = 1.000\n",
      "Step 15400, training acc = 1.000\n",
      "Step 15500, training acc = 1.000\n",
      "Step 15600, training acc = 1.000\n",
      "Step 15700, training acc = 0.980\n",
      "Step 15800, training acc = 1.000\n",
      "Step 15900, training acc = 1.000\n",
      "Step 16000, training acc = 1.000\n",
      "Step 16100, training acc = 1.000\n",
      "Step 16200, training acc = 1.000\n",
      "Step 16300, training acc = 1.000\n",
      "Step 16400, training acc = 1.000\n",
      "Step 16500, training acc = 1.000\n",
      "Step 16600, training acc = 1.000\n",
      "Step 16700, training acc = 1.000\n",
      "Step 16800, training acc = 1.000\n",
      "Step 16900, training acc = 1.000\n",
      "Step 17000, training acc = 1.000\n",
      "Step 17100, training acc = 1.000\n",
      "Step 17200, training acc = 1.000\n",
      "Step 17300, training acc = 1.000\n",
      "Step 17400, training acc = 1.000\n",
      "Step 17500, training acc = 1.000\n",
      "Step 17600, training acc = 1.000\n",
      "Step 17700, training acc = 1.000\n",
      "Step 17800, training acc = 1.000\n",
      "Step 17900, training acc = 1.000\n",
      "Step 18000, training acc = 1.000\n",
      "Step 18100, training acc = 1.000\n",
      "Step 18200, training acc = 1.000\n",
      "Step 18300, training acc = 1.000\n",
      "Step 18400, training acc = 1.000\n",
      "Step 18500, training acc = 1.000\n",
      "Step 18600, training acc = 1.000\n",
      "Step 18700, training acc = 1.000\n",
      "Step 18800, training acc = 1.000\n",
      "Step 18900, training acc = 1.000\n",
      "Step 19000, training acc = 1.000\n",
      "Step 19100, training acc = 1.000\n",
      "Step 19200, training acc = 1.000\n",
      "Step 19300, training acc = 0.980\n",
      "Step 19400, training acc = 1.000\n",
      "Step 19500, training acc = 1.000\n",
      "Step 19600, training acc = 1.000\n",
      "Step 19700, training acc = 1.000\n",
      "Step 19800, training acc = 1.000\n",
      "Step 19900, training acc = 1.000\n",
      "Test accuracy: 99.20%\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess3 = tf.InteractiveSession()\n",
    "sess3.run(tf.initialize_all_variables())\n",
    "\n",
    "# Running training step\n",
    "numSteps = 20000\n",
    "batchSize = 50\n",
    "for i in range(numSteps):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    #batch_x = tf.reshape(batch[0], [-1,28,28,1])\n",
    "    if i%100==0:\n",
    "        train_acc = accFunc.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob:1.0})\n",
    "        print('Step {0}, training acc = {1:.3f}'.format(i, train_acc))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob:0.5})\n",
    "    \n",
    "    \n",
    "\n",
    "testPerf_acc = accFunc.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob:1.0})\n",
    "print('Test accuracy: {0:.2f}%'.format(100*testPerf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing some workings\n",
    "\n",
    "Basic idea from: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "Writing out some of the math explicitly; \n",
    "Showing how can reshape 3D tensors into 2D matrices and perform dot products between filter and input to produce same result (i.e., filtering is essentially a dot product between input and learned weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to help\n",
    "\n",
    "def convertImage2ColVec(x, F, P, S):\n",
    "    \"\"\"\n",
    "    Converts 3D image into form in which dot products can \n",
    "    be calculated\n",
    "    (to illustrate approach from CS231n)\n",
    "    \n",
    "    NOTE: image already zero-padded at this point\n",
    "    \n",
    "    x: tensor (w0, h0, d0)\n",
    "        Input image\n",
    "    \n",
    "    F: int\n",
    "        receptive field\n",
    "        \n",
    "    P: int\n",
    "        Amount of zero padding to use around image\n",
    "        \n",
    "    S: int\n",
    "        Stride size\n",
    "    \n",
    "    \"\"\"\n",
    "    w0p,h0p,d0 = np.shape(x)  # already padded\n",
    "    w0 = w0p-2*P\n",
    "    w1 = int(sizeNextLayer(w0,F,P,S))\n",
    "    numR_new = F*F*d0\n",
    "    numC_new = w1*w1\n",
    "    x_col = np.zeros((numR_new, numC_new))  # new matrix to hold output\n",
    "    i2=0\n",
    "    for old_i in range(0,w1,S):\n",
    "        for old_j in range(0,w1,S):\n",
    "            newSlice = x[old_i:old_i+F,old_j:old_j+F,:]\n",
    "            x_col[:,i2] = np.ravel(newSlice.reshape(numR_new,-1))\n",
    "            i2+=1\n",
    "    return x_col\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 784)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFXcRkQhScRcRiaD/D0AOJsdIjb4pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111b62890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exInd = 7  # indice to choose from mnist\n",
    "w0 = 28   # original width of image in pixels\n",
    "P=1   # amount of padding\n",
    "F = 3  # receptive field of neuron\n",
    "S = 1   # stride\n",
    "d0 = 1    # original depth of image (i.e., number of channels)\n",
    "\n",
    "# get example image\n",
    "exImage = mnist.train.images[exInd,:].reshape(w0,-1)  # 28x28 pixels in each image\n",
    "exImage_3D = mnist.train.images[exInd,:].reshape(w0,-1, 1)  # 28x28 pixels in each image\n",
    "\n",
    "# pad image with zeros \n",
    "exImage_padded = np.pad(exImage,P,'constant', constant_values=0)\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(exImage_padded, cmap='Blues')\n",
    "np.shape(exImage_padded)\n",
    "# reshape to 3D\n",
    "exImage_padded_3D = exImage_padded.reshape(w0+2*P,-1,1)\n",
    "np.shape(exImage_padded_3D)\n",
    "\n",
    "w2 = sizeNextLayer(w0,F,P,S)  # get size of next layer\n",
    "\n",
    "# convert to 2D matrix, using function above\n",
    "exImage_cols = convertImage2ColVec(exImage_padded_3D, F=F, P=P, S=S)\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(exImage_cols, cmap='Blues')\n",
    "\n",
    "np.shape(exImage_cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1145adc50>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11186d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustrate effect of applying filter/conv layer\n",
    "\n",
    "# Set some weights\n",
    "# say single filter\n",
    "Wr_set = np.array([[-1,0,-1],[0,0,0],[-1,0,-1]])\n",
    "K = 1   # number of filters\n",
    "\n",
    "# perform dot product on 2D matrices\n",
    "Wr = np.reshape(Wr_set, (K, F*F*d0) ) # size K x (F*F*d0)\n",
    "Xc = exImage_cols  # size (F*F*d0) x (sizeNextLayer**2)\n",
    "dot_out = np.dot(Wr, Xc)\n",
    "\n",
    "np.shape(dot_out)\n",
    "\n",
    "# reshape back into 3D\n",
    "out_layer = np.reshape(dot_out,(w2, w2, K))\n",
    "\n",
    "# Plot (showing only a single channel)\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(exImage, cmap='Blues')\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(out_layer[:,:,0], cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick aside: how to print!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess4 = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17 35]\n",
      " [39 73]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3], [4,3,5]])\n",
    "b = np.array([[5,3,2], [7,5,6]])\n",
    "\n",
    "c = np.dot(a,b.T)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 35],\n",
       "       [39, 73]], dtype=int32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1,2,3,4,3,5], shape=[2,3])\n",
    "b = tf.constant([5,3,2,7,5,6], shape = [2,3])\n",
    "c = tf.matmul(a, b, transpose_b=True)\n",
    "c = tf.Print(c, [c], message=\"Printing c\")\n",
    "# Have to evaluate and be in a session in order to see output\n",
    "c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
